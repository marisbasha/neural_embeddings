{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import secrets\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from datasets import load_dataset, Audio\n",
    "import ctranslate2\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "import sys\n",
    "sys.path.append('./WhisperSeg')\n",
    "from WhisperSeg.model import WhisperSegmenterFast\n",
    "\n",
    "# Load configuration from YAML file\n",
    "with open('config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# Extract configuration values\n",
    "hf_dataset = config['hf_dataset']\n",
    "data_dir = config['data_dir']\n",
    "plot_dir = config['plot_dir']\n",
    "whisperseg_config = config['whisperseg_config']\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(hf_dataset, split='train')\n",
    "\n",
    "# Initialize models\n",
    "segmenter = WhisperSegmenterFast(\"Systran/faster-whisper-large-v2\", device=\"cpu\")\n",
    "encodec_model = EncodecModel.encodec_model_24khz()\n",
    "\n",
    "def generate_random_id(length):\n",
    "    alphabet = string.ascii_letters + string.digits\n",
    "    return ''.join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "def generate_features(audio, sr, min_freq, spec_time_step, num_trials):\n",
    "    ftr = segmenter.get_sliced_audios_features(audio, sr, min_freq, spec_time_step, num_trials)\n",
    "    features = ctranslate2.StorageView.from_array(np.asarray([ftr[0][2]]))\n",
    "    mel = ftr[0][2]\n",
    "    encoded = segmenter.model_list[0].encode(features)\n",
    "    embedding = torch.tensor(np.array(encoded).tolist(), device=\"cpu\")\n",
    "    \n",
    "    audio = torch.tensor(audio)\n",
    "    wav = convert_audio(audio, sr, encodec_model.sample_rate, encodec_model.channels)\n",
    "    wav = wav.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        encoded_frames = encodec_model.encode(wav)\n",
    "    codecs = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
    "    \n",
    "    return mel, embedding, codecs\n",
    "\n",
    "def process_subset(subset, config):\n",
    "    features_dir = os.path.join(data_dir, subset, 'features')\n",
    "    os.makedirs(features_dir, exist_ok=True)\n",
    "    for subdir in ['spectrograms', 'whisper_embeddings', 'encodec_codecs']:\n",
    "        os.makedirs(os.path.join(features_dir, subdir), exist_ok=True)\n",
    "\n",
    "    processed_data = []\n",
    "    subset_data = dataset.filter(lambda s: s['subset'] == subset)\n",
    "\n",
    "    for item in tqdm(subset_data, desc=f\"Processing {subset}\"):\n",
    "        try:\n",
    "            random_id = generate_random_id(32)\n",
    "            mel, embedding, codecs = generate_features(\n",
    "                item['audio']['array'],\n",
    "                item['audio']['sampling_rate'],\n",
    "                config['min_freq'], \n",
    "                config['spec_time_step'], \n",
    "                config['num_trials']\n",
    "            )\n",
    "\n",
    "            mel_file = os.path.join(features_dir, 'spectrograms', f'{random_id}.pt')\n",
    "            embedding_file = os.path.join(features_dir, 'whisper_embeddings', f'{random_id}.pt')\n",
    "            codecs_file = os.path.join(features_dir, 'encodec_codecs', f'{random_id}.pt')\n",
    "\n",
    "            torch.save(mel, mel_file)\n",
    "            torch.save(embedding, embedding_file)\n",
    "            torch.save(codecs, codecs_file)\n",
    "\n",
    "            processed_data.append({\n",
    "                'speaker': item.get('speaker', 'None'),\n",
    "                'type': item.get('type', 'utterance'),\n",
    "                'text': item['text'],\n",
    "                'mel': mel_file,\n",
    "                'embedding': embedding_file,\n",
    "                'codecs': codecs_file,\n",
    "                'waveform': item['audio']['path']\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing item in {subset}: {e}')\n",
    "\n",
    "    with open(os.path.join(features_dir, 'dataset.json'), 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "subsets = set(dataset['subset'])\n",
    "\n",
    "# Process each subset\n",
    "for subset in subsets:\n",
    "    if subset in whisperseg_config['songbirds']['datasets']:\n",
    "        config = whisperseg_config['songbirds']\n",
    "    elif subset in whisperseg_config['humans']['datasets']:\n",
    "        config = whisperseg_config['humans']\n",
    "    else:\n",
    "        print(f\"Skipping unknown subset: {subset}\")\n",
    "        continue\n",
    "    \n",
    "    process_subset(subset, config)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[0]\n",
    "audio, sr, min_freq, spec_time_step, num_trials = item['audio']['array'], item['audio']['sampling_rate'], config['min_freq'], config['spec_time_step'],  config['num_trials']\n",
    "ftr = segmenter.get_sliced_audios_features(audio, sr, min_freq, spec_time_step, num_trials)\n",
    "features = ctranslate2.StorageView.from_array(np.asarray([ftr[0][2]]))\n",
    "mel = ftr[0][2]\n",
    "encoded = segmenter.model_list[0].encode(features)\n",
    "embedding = torch.tensor(np.array(encoded).tolist(), device=\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = segmenter.model_list[0].encode(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ctranslate2._ext.StorageView at 0x20b4558dd70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.5442839 , -0.47230586, -0.1608399 , ..., -3.0254629 ,\n",
       "         -0.5218957 , -1.1900226 ],\n",
       "        [ 1.0759406 , -0.36049455,  0.8323346 , ..., -2.6637006 ,\n",
       "         -0.47056672, -0.29163405],\n",
       "        [ 1.4053938 ,  0.7581593 ,  0.5551884 , ..., -2.3033926 ,\n",
       "         -0.41483313, -0.643653  ],\n",
       "        ...,\n",
       "        [ 0.9287099 , -0.6650456 ,  0.26375106, ...,  0.3682955 ,\n",
       "          0.9621666 , -0.2075649 ],\n",
       "        [ 0.9439007 , -0.97815657,  0.43080607, ...,  0.68621343,\n",
       "          1.2327944 , -0.23502705],\n",
       "        [ 0.9160954 , -0.63507974,  0.41857827, ...,  0.8747503 ,\n",
       "          1.6396738 ,  0.04554638]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2ndp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
